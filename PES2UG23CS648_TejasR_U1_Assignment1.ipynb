{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrIt5VpHNSCmLAtqk+ho2F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejas-techstack/genai/blob/main/PES2UG23CS648_TejasR_U1_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xWGjO4LTQpnu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0df43e3e-8774-4d56-f3bb-c64c744cc808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, set_seed\n",
        "set_seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generation_prompt = \"The future of Artificial Intelligence is\"\n",
        "\n",
        "mask_sentence_bert = \"The goal of Generative AI is to [MASK] new content.\"\n",
        "mask_sentence_roberta = \"Generative AI will change the <mask> of technology.\"\n",
        "mask_sentence_bart = \"The goal of Generative AI is to <mask> new content.\"\n",
        "\n",
        "context = \"\"\"\n",
        "Generative AI poses significant risks such as hallucinations, bias, and deepfakes.\n",
        "\"\"\"\n",
        "\n",
        "question = \"What are the risks?\""
      ],
      "metadata": {
        "id": "kizQ1f7vyZNH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Generation"
      ],
      "metadata": {
        "id": "EB1bNW8yyQdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_gen = pipeline(\"text-generation\", model=\"bert-base-uncased\")\n",
        "out = bert_gen(generation_prompt, max_length=40)\n",
        "print(out[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYbldbi5y7Pq",
        "outputId": "fcff632d-eaaa-4315-c4ef-162e1fbe90b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is................................................................................................................................................................................................................................................................\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_gen = pipeline(\"text-generation\", model=\"roberta-base\")\n",
        "out = roberta_gen(generation_prompt, max_length=40)\n",
        "print(out[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "9qV4Nq_20Qm6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f08ac802-55d7-473b-efe3-72a6fd141e25"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bart_gen = pipeline(\"text-generation\", model=\"facebook/bart-base\")\n",
        "bart_out = bart_gen(generation_prompt, max_length=40)\n",
        "print(bart_out[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYBMzZKIx9go",
        "outputId": "1c705b9a-98ca-4ac9-af80-b51fe084c833"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForCausalLM were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['lm_head.weight', 'model.decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The future of Artificial Intelligence isOtherwise ShakOtherwise sure Shak Shak323208 df empir squat Shak chuckms healer df MarxismOtherwisePatrick Walkdy Shak slipsreleased df df32 slips debugger Walk slips Shak denim Shak df df df Walk Drawn Drawn Drawn 361 slipsino Person Princeton Shak chuckSpoilerSpoiler slips df df initially dismant Drawn Crkas Drawn Drawn spotsenc Shak Drawn Drawnino Drawn Drawn opposition Shak workload slips Shak slips df Drawn DrawnEngland Drawn32 Drawn DrawnSel Drawn workload Drawn Drawn Output Drawn Drawn communism Drawn Drawn debugger spots spots dfPost df df workload Drawn df Drawn dfJe Drawn DrawnaxeFrames df kingdoms df Drawnlein Drawn Drawn slips Drawn Drawneller Drawn Drawn df spots spots game Drawn Drawn LT Drawn Drawn Alvin Drawn origins sure Drawn Drawnaze spots Drawn Drawn princip Drawn gameStatus Alvin df df Alvin df impacting impacting spots spotsaily princip futures df principAlert skysc df spots principStatus Drawn Drawn Beet Drawn skysc Alvin df game sure Alvin workloadStatus Molecular princip debuggerkaskasFramesFrames Drawn Drawn origins df df princip Drawn DrawnwalkingFrameskas workload Drawnkasbreak Drawn Drawn2014 Hook df Drawnkas tables df Beet game df Drawn skysc Drawn debuggerkas spots Drawnaxekas impactingkaskas Beet20142014kas debugger impacting df df futures debuggerAlert originskas originskaskas df debugger skysckas Beet Beet debugger spots df elephants df df game game\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fill Mask"
      ],
      "metadata": {
        "id": "4zQo-uuQyU_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_fill = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
        "bert_preds = bert_fill(mask_sentence_bert)\n",
        "for p in bert_preds[:3]:\n",
        "    print(p[\"token_str\"], round(p[\"score\"], 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKjooAN4x9mn",
        "outputId": "adb8ec5b-2014-45de-c06c-99f0d7728b15"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "create 0.54\n",
            "generate 0.156\n",
            "produce 0.054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_fill = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
        "roberta_preds = roberta_fill(mask_sentence_roberta)\n",
        "for p in roberta_preds[:3]:\n",
        "    print(p[\"token_str\"], round(p[\"score\"], 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9sWkxWfx9sj",
        "outputId": "ff50aea0-333c-468a-a685-53881cf80bec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " face 0.626\n",
            " future 0.135\n",
            " world 0.07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bart_fill = pipeline(\"fill-mask\", model=\"facebook/bart-base\")\n",
        "bart_preds = bart_fill(mask_sentence_bart)\n",
        "for p in bart_preds[:3]:\n",
        "    print(p[\"token_str\"], round(p[\"score\"], 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jOYKaDtx9yM",
        "outputId": "b50c41bb-1da7-43e7-b98d-ca511c424a1b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " create 0.075\n",
            " help 0.066\n",
            " provide 0.061\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question Answering"
      ],
      "metadata": {
        "id": "y7NO40k3yX2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_qa = pipeline(\"question-answering\", model=\"bert-base-uncased\")\n",
        "bert_ans = bert_qa(question=question, context=context)\n",
        "print(bert_ans[\"answer\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJcYb5ZQx930",
        "outputId": "cf32c821-6e71-45fd-a35f-978c6de4c23a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hallucinations, bias, and deepfakes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roberta_qa = pipeline(\"question-answering\", model=\"roberta-base\")\n",
        "roberta_ans = roberta_qa(question=question, context=context)\n",
        "print(roberta_ans[\"answer\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u1t0T8xx99c",
        "outputId": "640fe791-0008-42bf-901c-8eed65ae500d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bart_qa = pipeline(\"question-answering\", model=\"facebook/bart-base\")\n",
        "bart_ans = bart_qa(question=question, context=context)\n",
        "print(bart_ans[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfYKTkyyx-DD",
        "outputId": "225a24bb-2b3e-4d53-fb1a-be09a176a7f2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BartForQuestionAnswering were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generative AI poses significant risks such as\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Task | Model | Success / Failure | Observation | Reason |\n",
        "|------|-------|------------------|-------------|--------|\n",
        "| **Generation** | BERT | Failure | Could not generate proper text. | Encoder model, not meant for generation |\n",
        "| | RoBERTa | Failure | Output was not meaningful | Encoder-only architecture |\n",
        "| | BART | Success | Generated text but quality was limited | Has encoder and decoder, supports generation |\n",
        "| **Fill-Mask** | BERT | Success | Predicted correct words like create | Trained using masked words |\n",
        "| | RoBERTa | Success | Gave relevant predictions | Masked language training |\n",
        "| | BART | Success | Predictions were acceptable | Not mainly trained for this task |\n",
        "| **QA** | BERT | Success | Correct answer found from context | Good at understanding texts |\n",
        "| | RoBERTa | Success | Output was weak or unreliable. | Not trained specifically for Q/A |\n",
        "| | BART | Success | Returned only part of the answer | QA not optimized in base model |\n"
      ],
      "metadata": {
        "id": "pAYNINEMyklC"
      }
    }
  ]
}