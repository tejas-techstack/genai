{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejas-techstack/genai/blob/main/PES2UG23CS648_TejasR_GI_handson1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "w_qwREnttBgE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cead777-5f0b-4272-e1e1-c799becfd70f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "import transformers\n",
        "import nltk\n",
        "import tensorflow\n",
        "\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7IfaxCT3tWed",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e57be63-dbb4-4628-d2a1-fa250eaeec12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative AI and Its Applications: A Foundational Briefing\n",
            "\n",
            "Executive Summary\n",
            "\n",
            "This document provides a comprehensive overview of Generative AI, synthesizing foundational concepts, technological underpinnings, and practical applications as outlined in the course materials from PES University. Generative AI represents a transformative subset of Artificial Intelligence focused on creating novel content, a capability primarily driven by the advent of Large Language Models (LLMs). The evolution of these models, from early rule-based systems to the sophisticated, multi-billion parameter architectures of today like GPT-4, is rooted in the development of the Transformer architecture in 2017. This architecture, with its revolutionary \"attention mechanism,\" enables models to understand context and long-range dependencies in data with unprecedented accuracy.\n",
            "\n",
            "Building an LLM is a two-stage process involving pretraining on vast, diverse datasets to build a general understanding of language, followed by finetuning to specialize the model for specific tasks. The functionality of these models relies on core principles of Natural Language Processing (NLP), such as converting text into numerical vectors via word embeddings and understanding grammatical structure through Part-of-Speech (POS) tagging and Named Entity Recognition (NER).\n",
            "\n",
            "The document explores the diverse ecosystem of LLMs, including multimodal models that process images and audio, and the growing trend of smaller, open-source models. It also delves into practical applications in academia and beyond, while addressing the critical risks associated with this technology, including data privacy, intellectual property, and academic integrity. The final section details the specific structure, evaluation policies, and project-based learning approach of the \"Generative AI and Its Applications\" course, providing a complete picture of its academic framework.\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "1. The Landscape of Artificial Intelligence\n",
            "\n",
            "To understand Generative AI, it is essential to first grasp its position within the broader field of Artificial Intelligence. The relationship between these disciplines is hierarchical, with each term representing a more specialized subset of the last.\n",
            "\n",
            "1.1 Defining the Terms\n",
            "\n",
            "* Artificial Intelligence (AI): In its broadest sense, AI refers to the simulation of human intelligence processes by computer systems. It is the overarching field dedicated to creating machines that can think, learn, and problem-solve.\n",
            "* Machine Learning (ML): A subset of AI, Machine learning focuses on designing specific systems that can learn from and make decisions or predictions based on data, without being explicitly programmed for the task.\n",
            "* Deep Learning (DL): A subset of Machine Learning that utilizes a specific set of algorithms known as neural networks, often composed of many layers (hence \"deep\"). Deep Learning has been the driving force behind recent breakthroughs in AI, including in areas like Natural Language Processing (NLP), Generative Adversarial Networks (GANs), and Transformers.\n",
            "* Generative AI (GenAI): A branch of Deep Learning that focuses on creating new, original content. This includes text, images, audio, and more. Large Language Models (LLMs) are a key component of modern Generative AI.\n",
            "\n",
            "This hierarchy can be visualized as follows: Machine Learning (ML) -> Deep Learning (DL) -> {NLP, GAN, Transformers, GenAI} -> Large Language Models (LLM)\n",
            "\n",
            "1.2 Core Machine Learning Paradigms\n",
            "\n",
            "Machine Learning models learn from data in several fundamental ways, each suited to different types of problems.\n",
            "\n",
            "Supervised Learning\n",
            "\n",
            "In supervised learning, the model is trained on labeled data, meaning each data point is tagged with a correct output or label. The goal is to learn a mapping function that can predict the output for new, unseen data.\n",
            "\n",
            "An example is an Email Spam Classifier:\n",
            "\n",
            "* Data: A collection of emails explicitly tagged as \"Spam\" or \"Not Spam\".\n",
            "* Training & Inference: The model can be trained in two ways:\n",
            "  * Discriminative Approach: The model learns the decision boundary that separates the two classes. During inference, it determines on which side of the boundary a new email falls.\n",
            "  * Generative Approach: The model learns the underlying distribution of the data for each class (i.e., what \"spam\" emails look like and what \"not spam\" emails look like). During inference, it calculates the likelihood that a new email belongs to each class based on these learned distributions.\n",
            "\n",
            "Unsupervised Learning\n",
            "\n",
            "Unsupervised learning models work with un-labeled data. The objective is to identify hidden patterns, structures, and relationships within the data without any predefined labels.\n",
            "\n",
            "An example is Email Topic Modeling:\n",
            "\n",
            "* Data: A large collection of emails without any categorization.\n",
            "* Training: The model learns the distribution that generates the structure within the data, grouping similar emails into clusters.\n",
            "* Inference: A new email is assigned to the cluster where it has the highest probability of belonging.\n",
            "\n",
            "Reinforcement Learning\n",
            "\n",
            "This paradigm involves an agent that learns to make decisions by interacting with an environment. The agent's goal is to maximize a cumulative reward over time.\n",
            "\n",
            "The process involves a continuous loop:\n",
            "\n",
            "1. Interaction: The agent chooses an action based on its current state and policy (strategy).\n",
            "2. Reward/Penalty: After each action, the environment provides feedback in the form of a reward or penalty.\n",
            "3. Policy Update: The agent updates its policy based on the feedback, reinforcing actions that lead to positive rewards and avoiding those that lead to penalties.\n",
            "  * Example: A self-driving car (the agent) learns to navigate traffic (the environment) by receiving positive rewards for obeying rules and safely reaching its destination, and penalties for errors.\n",
            "\n",
            "Shallow and Deep Models\n",
            "\n",
            "* Shallow Models: Models with a limited number of layers, capable of capturing only linear and simple non-linear relationships.\n",
            "* Deep Models: Models with many layers, capable of capturing complex, hierarchical patterns in data.\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "2. Introduction to Generative AI and Large Language Models (LLMs)\n",
            "\n",
            "Generative AI is a revolutionary technology that shifts the focus from analyzing existing data to creating entirely new data. At its heart are Large Language Models, the engines that power many of these capabilities.\n",
            "\n",
            "2.1 How Generative AI Works\n",
            "\n",
            "Generative AI models work by learning the patterns and structures of their training data and then using that knowledge to generate new content. When generating text, an LLM takes an input sequence of words (tokens) and predicts the next most probable word. This process is repeated, with each newly generated word becoming part of the input for predicting the next, allowing the model to produce coherent sentences, paragraphs, and even entire documents.\n",
            "\n",
            "A key challenge in this process is \"Hallucinations\"—instances where the model's output is nonsensical, factually incorrect, or disconnected from the input prompt. This occurs because the model is generating content based on statistical probabilities rather than true understanding or knowledge. This underscores the importance of critically evaluating all AI-generated output.\n",
            "\n",
            "2.2 The Rise of Large Language Models (LLMs)\n",
            "\n",
            "The term \"Large\" in LLMs refers to two key aspects:\n",
            "\n",
            "1. Architecture Size: These models have an enormous number of internal parameters, which are like the knobs and switches the model tunes during training. Popular models like GPT-3 have hundreds of billions of parameters.\n",
            "2. Training Data: They are trained on vast amounts of text and other data, allowing them to capture incredibly complex patterns in language and the world.\n",
            "\n",
            "The growth of LLMs has been explosive, particularly since the introduction of the Transformer architecture. The number of parameters in state-of-the-art models has grown exponentially, from millions to hundreds of billions and even trillions. This scaling has been a primary driver of their increased capabilities.\n",
            "\n",
            "2.3 A Timeline of Evolution\n",
            "\n",
            "The journey from simple rule-based systems to modern LLMs spans decades, with an acceleration of progress in recent years.\n",
            "\n",
            "Year/Era\tKey Developments\n",
            "1950s-1960s\tRule-Based Language Models (e.g., Eliza in 1967)\n",
            "1980s-1990s\tStatistical Language Processing, N-gram Models, RNN, LSTM\n",
            "2017\tTransformers (\"Attention is all you need\")\n",
            "2018\tBERT, GPT-1\n",
            "2019\tGPT-2, RoBERTa, XLNet\n",
            "2020\tGPT-3\n",
            "2021\tGPT-3.5\n",
            "2022\tInstructGPT, ChatGPT released to the public\n",
            "2023\tGPT-4, LLaMa, PaLM 2, Falcon\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "3. The Transformer Architecture: The Engine of Modern LLMs\n",
            "\n",
            "The introduction of the Transformer architecture in the 2017 paper \"Attention is all you need\" was a watershed moment in AI. It provided a more effective and scalable way to handle sequential data like text, replacing older, less efficient methods like recurrence (RNNs) and convolutions.\n",
            "\n",
            "3.1 The Attention Mechanism\n",
            "\n",
            "The fundamental innovation of the Transformer is the attention mechanism. This component allows the model to weigh the importance of different words (tokens) in the input sequence when making a prediction. In essence, for each word it processes, the model can \"pay attention\" to all other words in the input, helping it understand context, resolve ambiguity, and handle long-range dependencies. This is crucial for tasks like translation, summarization, and question answering.\n",
            "\n",
            "The Transformer architecture consists of an encoder stack (to process the input) and a decoder stack (to generate the output), both of which heavily utilize multi-head attention and feed-forward networks.\n",
            "\n",
            "3.2 The Evolution of the GPT Series\n",
            "\n",
            "The Generative Pre-trained Transformer (GPT) series from OpenAI is a prime example of how scaling the Transformer architecture has led to progressively more powerful models.\n",
            "\n",
            "Version\tParameters\tTraining Data\tKey Innovations & Capabilities\n",
            "GPT-1\t117 Million\tBooksCorpus\tEstablished the paradigm of unsupervised pre-training and supervised fine-tuning.\n",
            "GPT-2\t1.5 Billion\tWebText (8 million documents)\tDemonstrated strong zero-shot performance, showing the power of scaling.\n",
            "GPT-3\t175 Billion\tCommon Crawl, WebText, Books, Wikipedia\tImpressive few-shot and zero-shot capabilities; introduced in-context learning.\n",
            "GPT-3.5\t~175 Billion\tUpdated and expanded datasets\tImproved performance, reliability, and alignment with user intent.\n",
            "GPT-4\tTrillions (est.)\tVastly expanded and more diverse multimodal datasets\tSuperior performance, multimodality (handling images), increased robustness.\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "4. Building and Diversifying LLMs\n",
            "\n",
            "Creating a powerful LLM is a complex, resource-intensive process that occurs in two distinct stages.\n",
            "\n",
            "4.1 The Two-Stage Construction Process\n",
            "\n",
            "1. Pretraining: This is the initial, computationally expensive phase. The model is trained on a massive, diverse corpus of raw text (billions of tokens from books, websites, etc.). The goal is to develop a broad, general understanding of language by learning to predict the next word in a sequence. The output of this stage is a base model or foundation model.\n",
            "2. Finetuning: In this stage, the pre-trained base model is further trained on a smaller, curated dataset specific to a particular task (e.g., medical question answering, legal document summarization). This adapts the model's general language capabilities to a specialized domain, improving its performance and alignment with desired behaviors.\n",
            "\n",
            "4.2 The Expanding LLM Ecosystem\n",
            "\n",
            "The field is evolving beyond monolithic, text-only models.\n",
            "\n",
            "* Multimodal LLMs: These models represent a significant advance, extending beyond text to process and understand multiple forms of input—such as images, audio, and video—simultaneously. This brings AI closer to a more human-like perception of the world. Examples include GPT-4 and LLaVA.\n",
            "* Small, Local, and Open-Source LLMs: There is a growing trend towards smaller, more efficient LLMs designed to run on personal hardware (\"edge devices\") without cloud dependency. Open-source models like Llama 3.3, Phi 3, and Mistral are gaining popularity as they allow for greater customization, transparency, and community-driven development. Frameworks like Ollama simplify the process of running these models locally.\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "5. Fundamentals of Natural Language Processing (NLP)\n",
            "\n",
            "LLMs are built upon a foundation of NLP, the field of AI concerned with enabling computers to understand, interpret, and generate human language.\n",
            "\n",
            "5.1 Preparing Text for Machines\n",
            "\n",
            "Computers cannot process raw text; it must be converted into a numerical format.\n",
            "\n",
            "* Tokenization: This is the first step, where a text is broken down into smaller units called tokens. These can be words, subwords, or characters. This process, also known as word segmentation, is crucial for defining the model's vocabulary.\n",
            "* Word Embeddings: After tokenization, each token is mapped to a numerical vector. This is done using a word-embedding layer, which acts as a lookup table. These vectors are not random; they are learned in a way that captures the semantic meaning and relationships between words. For example, the vectors for \"king\" and \"queen\" would be mathematically closer to each other than to the vector for \"car.\" A good numerical representation should have semantic meaning and be informative.\n",
            "\n",
            "5.2 Understanding Language Structure and Meaning\n",
            "\n",
            "NLP employs various techniques to analyze the grammatical structure and meaning of text.\n",
            "\n",
            "* Parts-of-Speech (POS) Tagging: This is the process of assigning a grammatical category (e.g., Noun (N), Verb (V), Adjective (ADJ)) to each word in a sentence. POS tags are essential features for many downstream NLP tasks like NER and parsing, as they help resolve ambiguity (e.g., whether \"book\" is a noun or a verb).\n",
            "* Named Entity Recognition (NER): NER is the task of identifying and classifying named entities in text into predefined categories. This is more complex than POS tagging as it involves identifying spans of text.\n",
            "\n",
            "Entity Type\tDescription\tExamples\n",
            "ENAMEX\tNamed entities (Persons, Organizations, Locations)\t\"Robin\", \"HCL\", \"Chennai\"\n",
            "NUMEX\tNumerical expressions (Money, Distance, Count)\t\"$160\", \"twenty feet\", \"12 students\"\n",
            "TIMEX\tTemporal expressions (Date, Time, Period)\t\"August 15 1947\", \"9.30 a.m.\", \"17th century\"\n",
            "\n",
            "5.3 The Challenge of Ambiguity\n",
            "\n",
            "Human language is inherently ambiguous, posing a significant challenge for NLP systems.\n",
            "\n",
            "* Lexical Ambiguity: A single word has multiple meanings (e.g., \"bat\" can be an animal or a piece of sports equipment).\n",
            "* Syntactic Ambiguity: A sentence can be parsed in multiple ways (e.g., in \"I heard his cell phone ring in my office,\" it is unclear if the phone was in the office or if the hearing took place in the office).\n",
            "* Anaphoric Ambiguity: A pronoun or referring expression could refer to more than one previously mentioned entity (e.g., in \"Margaret invited Susan for a visit, and she gave her a good lunch,\" who is \"she\" and who is \"her\"?).\n",
            "* Metonymy: A phrase's figurative meaning differs from its literal one (e.g., \"Samsung is screaming for new management\").\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "6. A Classic NLP Task: Text Classification\n",
            "\n",
            "Text classification is a foundational NLP task that involves assigning a predefined category or label to a piece of text. A common algorithm for this is the Naive Bayes Classifier.\n",
            "\n",
            "6.1 The Naive Bayes Classifier\n",
            "\n",
            "This classifier uses Bayes' theorem to determine the probability of a text belonging to a certain class, given the words it contains.\n",
            "\n",
            "* Core Principle: It calculates the most probable class (v_MAP) for a given set of attributes (words) (a1, a2, ... an) by finding the class v_j that maximizes the posterior probability P(v_j | a1, a2, ... an).\n",
            "* The \"Naive\" Assumption: The key simplification of this model is the assumption that all features (words) are conditionally independent of each other, given the class. This means it assumes the presence of one word does not affect the presence of another, which is not true in language but provides a simple and often effective model.\n",
            "* The Zero Frequency Problem: If a word in the test data never appeared in the training data for a particular class, its probability would be zero, causing the entire probability calculation for that class to become zero. This is solved using Laplace smoothing, which involves adding one to every word count to ensure no probability is ever zero.\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "7. The PES University Course: \"Generative AI and Its Applications\"\n",
            "\n",
            "The source material provides a detailed look into the structure and content of course UE22CS342BA9, taught by Dr. Pooja Agarwal in the Department of CSE at PES University.\n",
            "\n",
            "7.1 Course Structure and Syllabus\n",
            "\n",
            "The course is divided into four main units, each allocated 14 hours.\n",
            "\n",
            "Unit\tTitle\tKey Topics\n",
            "1\tLLM Architecture and Models\tLLM basics, NLP fundamentals (Word Embeddings, POS, NER), Text Classification, Transformer anatomy, architectures (BERT, GPT, ELMo, RoBERTa, BART), HuggingFace usage.\n",
            "2\tPrompt Engineering and RAG\tPrompting techniques (zero/few-shot), Chain/Tree/Graph of Thought, LangChain, Retrieval Augmented Generation (Naive and Advanced RAG), Q&A systems, bias/toxicity evaluation.\n",
            "3\tAgent and Multimodal LLM\tAgentic workflows (Microsoft Autogen, CrewAI), GANs, Diffusion networks, Image generation, Multimodal LLMs (CLIP, BLIP models).\n",
            "4\tFine Tuning\tLLM finetuning principles, data resolution, quantization, prompt tuning, finetuning a custom LLM, harms of LLMs, data, scaling, and privacy.\n",
            "\n",
            "* Tools/Languages: Python, HuggingFace, LM Studio, Kaggle.\n",
            "\n",
            "7.2 Evaluation and Assessment\n",
            "\n",
            "The student evaluation is split equally between In-Semester Assessment (ISA) and End-Semester Assessment (ESA).\n",
            "\n",
            "In-Semester Assessment (ISA) - Max 50 Marks\n",
            "\n",
            "Activity\tMarks\tRemarks\n",
            "ISA1\t15\tCBT mode\n",
            "ISA2\t15\tCBT mode\n",
            "Experiential Learning & Tutorials\t20\tComprised of a Project (10 marks), Hands-on Submissions (4 marks), Tutorial 1 (4 marks), Tutorial 2 (4 marks), and a Guest Lecture (2 marks).\n",
            "Total ISA\t50\t\n",
            "\n",
            "End-Semester Assessment (ESA) - Max 50 Marks\n",
            "\n",
            "Activity\tMarks\tRemarks\n",
            "ESA\t50\tFinal ESA exam\n",
            "Total ESA\t50\t\n",
            "\n",
            "7.3 Practical Application: Project Ideas\n",
            "\n",
            "The course emphasizes hands-on learning through a variety of project ideas, including:\n",
            "\n",
            "1. Text Generation (using GPT-2/3)\n",
            "2. Chatbot Development (using GPT-3/DialoGPT)\n",
            "3. Text Summarization (using BART/T5)\n",
            "4. Sentiment Analysis (using BERT/RoBERTa)\n",
            "5. Named Entity Recognition (NER) (using SpaCy/BERT)\n",
            "6. Language Translation (using MarianMT/M2M100)\n",
            "7. Text-Based Games (using GPT-2/3)\n",
            "8. Fake News Detection (using BERT/RoBERTa)\n",
            "9. Medical Question Answering System (using GPT-3)\n",
            "10. Symptom Checker (using GPT-3)\n",
            "11. Automated Essay Grader (using BERT/GPT-3)\n",
            "\n",
            "7.4 Policy on AI Tool Usage\n",
            "\n",
            "The course acknowledges the role of modern AI tools and establishes a clear policy for their use in assignments. Students are permitted to use tools like Stable Diffusion, ChatGPT, etc., provided they adhere to the following rules:\n",
            "\n",
            "1. Credit the tool used.\n",
            "2. Identify which part of the work was generated by the AI tool.\n",
            "3. Briefly summarize why the tool was used and include its output.\n",
            "\n",
            "This policy is framed by an awareness of the realistic risks of using Generative AI:\n",
            "\n",
            "* Violations of data privacy: Students may be uncomfortable providing data to create accounts for AI services.\n",
            "* Violations of intellectual property: Students should check if their inputs will be used as training data.\n",
            "* Violations of academic integrity: Students must not pass off AI-generated work as their own. The policy suggests using tools like OpenAI's AI Text Classifier or GPTZero to analyze submissions.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "filepath = \"lab.txt\"\n",
        "with open (filepath, \"r\") as file:\n",
        "    text = file.read()\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qID35ToNt2rI"
      },
      "outputs": [],
      "source": [
        "transformers.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Lir_3YS0uGDu"
      },
      "outputs": [],
      "source": [
        "prompt = \"Generative AI is a revolutionary technology that\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nbtymPlctgi6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f81c8b3b-c318-4ead-9d27-eda182c00d11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative AI is a revolutionary technology that is designed to work with existing AI systems. It has been developed by the University of California, Berkeley. Its research team is the leading developer of AI software and its use is limited to AI and AI systems.\n",
            "\n",
            "\n",
            "The research team led by Professor Daniel Kranz, from the University of California, Berkeley, has developed a program to learn how to use the AI to improve the performance of the software. It has been developed by the University of California, Berkeley. Its research team is the leading developer of AI software and its use is limited to AI and AI systems. It is a top-selling research computer software company, and is a top-selling research computer software company.\n",
            "The research team developed the program to learn how to use the AI to improve the performance of the software. It has been developed by the University of California, Berkeley, Berkeley, and is a top-selling research computer software company. The research team developed the program to learn how to use the AI to improve the performance of the software. It has been developed by the University of California, Berkeley, and is a top-selling research computer software company.\n",
            "The research team developed the program to learn how to use the AI to improve the performance of the software. It has been developed by\n"
          ]
        }
      ],
      "source": [
        "fast_generator = transformers.pipeline(\"text-generation\", model = \"distilgpt2\")\n",
        "\n",
        "output_fast = fast_generator(prompt, max_length = 50, num_return_sequences=1)\n",
        "print(output_fast[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "o2HstZ3st11f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015c7ae8-40cd-41cb-f776-355566dfdb27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Both `max_new_tokens` (=256) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generative AI is a revolutionary technology that allows users to build AI that can help solve complex problems. It brings together hundreds of different approaches to solve problems, from solving complex problems in a laboratory to solving complex problems in a city. The technology allows users to build a computer that makes decisions based on user input, not on intuition.\n",
            "\n",
            "The AI is a model of human intelligence, and has many aspects that are similar to artificial intelligence. It can learn from humans, and it can adapt to the environment. It can learn by experimenting with new ways of thinking, and it can learn by learning from its own experience.\n",
            "\n",
            "It is the main driving force behind the new Artificial Intelligence, and the AI is very important to the success of AI. The new AI is designed to work out problems that need to be solved in a way that is easy to understand and solve, and that is flexible enough to be easily adaptable to different environments.\n",
            "\n",
            "The AI is designed to be scalable and adaptable to different environments. It can be used to solve complex problems without relying on humans. It can be used to build a solution that is very quickly scalable, scalable, inexpensive, and adaptable to different environments.\n",
            "\n",
            "The new AI is designed to work out problems that need to be solved in a way that is\n"
          ]
        }
      ],
      "source": [
        "fast_generator = transformers.pipeline(\"text-generation\", model = \"gpt2\")\n",
        "\n",
        "output_fast = fast_generator(prompt, max_length = 50, num_return_sequences=1)\n",
        "print(output_fast[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OqG18Nvftfdb"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lxe34XJtvYqq"
      },
      "outputs": [],
      "source": [
        "sample_sentence = \"Transformers revolutionized NLP.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7CoM1aq3vg0V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bd985a9-5c61-48db-a78a-6b1d142012ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens: ['Transform', 'ers', 'Ġrevolution', 'ized', 'ĠN', 'LP', '.']\n"
          ]
        }
      ],
      "source": [
        "tokens = tokenizer.tokenize(sample_sentence)\n",
        "print(f\"tokens: {tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SxDVPfS7vrCk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "415239d2-cf5a-4ec0-d9c9-b863e55a625b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "token ids: [41762, 364, 5854, 1143, 399, 19930, 13]\n"
          ]
        }
      ],
      "source": [
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(f\"token ids: {token_ids}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "x54NJHVqxTEY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15c5388f-1337-4d00-bdd3-5f57f9244f8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mJCUpEhSxf1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fa4ed07-3ddb-4742-a100-175ebcf1f217"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos tags: [('Transformers', 'NNS'), ('revolutionized', 'VBD'), ('NLP', 'NNP'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "pos_tags = nltk.pos_tag(nltk.word_tokenize(sample_sentence))\n",
        "print(f\"pos tags: {pos_tags}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0VaiB0DAyRId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "717599b9-af37-4cf3-d79b-206ad57791d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER',\n",
              "  'score': np.float32(0.99842685),\n",
              "  'word': 'Sylvian',\n",
              "  'start': 11,\n",
              "  'end': 18}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "ner_pipeline = transformers.pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", aggregation_strategy=\"simple\")\n",
        "ner_pipeline(\"My name is Sylvian and I am work at hugging face\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "k1ycivmo0VoY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf0f50f4-db28-40b4-a56c-c12626aaecce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "entity                    | Type       | Score     \n",
            "--------------------------------------------------\n",
            "AI                        | MISC       | 0.98\n",
            "PES University            | ORG        | 0.99\n",
            "AI                        | MISC       | 0.98\n",
            "Large Language Models     | MISC       | 0.91\n",
            "LLMs                      | MISC       | 0.90\n",
            "Transformer               | MISC       | 0.99\n"
          ]
        }
      ],
      "source": [
        "snippet=text[:1000]\n",
        "entities = ner_pipeline(snippet)\n",
        "print(f\"{'entity':<25} | {'Type':<10} | {'Score':<10}\")\n",
        "print(\"-\"*50)\n",
        "for entity in entities:\n",
        "  if entity[\"score\"]>0.90:\n",
        "    print(f\"{entity['word']:<25} | {entity['entity_group']:<10} | {entity['score']:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-X6p3uH41fqy"
      },
      "outputs": [],
      "source": [
        "transformer_section = \"\"\"\n",
        "This is sentence A where sentence is a sentence\n",
        "I am an applebet the world comes crumbling down\n",
        "the alphabet tastes really nice. whater is good for sheep\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xxwdQLj0105W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd784457-a70f-4ae8-d4a8-902fa4a94ce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Your max_length is set to 60, but your input_length is only 38. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " This is sentence A where sentence is a sentence . The alphabet tastes really nice. whater is good for sheep . The world comes crumbling down. I am an apple .\n"
          ]
        }
      ],
      "source": [
        "fast_sum = transformers.pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
        "res_fast = fast_sum(transformer_section, max_length=60, min_length=30, do_sample=False)\n",
        "print(res_fast[0]['summary_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dbnaTVTfDau4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c7b2dd5-a5e0-4d27-c421-be06fe6dd28c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Your max_length is set to 60, but your input_length is only 38. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am an applebet the world comes crumbling down. The alphabet tastes really nice. whater is good for sheep.  This is sentence A where sentence is a sentence A.\n"
          ]
        }
      ],
      "source": [
        "smart_sum = transformers.pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "res_smart = smart_sum(transformer_section, max_length=60, min_length=30, do_sample=False)\n",
        "print(res_smart[0]['summary_text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzADb1wrV0vb",
        "outputId": "916be5b2-bb4c-473c-8b16-d5b284cd82ed"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What is the fundamental innovation of the Transformer?\",\n",
        "    \"What are the risks of using Generative AI?\"\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    res = qa_pipeline(question=q, context=text[:5000])\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    print(f\"A: {res['answer']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkbMAlNYV1R2",
        "outputId": "b2b073b1-f16c-490d-a2fa-38b930b282bf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Q: What is the fundamental innovation of the Transformer?\n",
            "A: to identify hidden patterns, structures, and relationships within the data\n",
            "\n",
            "Q: What are the risks of using Generative AI?\n",
            "A: data privacy, intellectual property, and academic integrity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdF4h-F6V667",
        "outputId": "fc71ea75-4ead-40ee-ffa5-9637338718d0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_sentence = \"The goal of Generative AI is to create new [MASK].\"\n",
        "preds = mask_filler(masked_sentence)\n",
        "\n",
        "for p in preds:\n",
        "    print(f\"{p['token_str']}: {p['score']:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBiQAgh3V9sE",
        "outputId": "0cc8379b-679c-4b97-ff3d-133ac8b22d58"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "applications: 0.06\n",
            "ideas: 0.05\n",
            "problems: 0.05\n",
            "systems: 0.04\n",
            "information: 0.03\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlUxQbAviWWBKHW2fh1gb8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}