{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPVv6FJybxNu5YtvXivsyX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejas-techstack/genai/blob/main/unit2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_google_genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0kq1E13wDPF",
        "outputId": "1d93138f-ce11-466f-9864-3b7df6dababb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_google_genai\n",
            "  Downloading langchain_google_genai-4.2.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain_google_genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.56.0 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (1.63.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.5 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (1.2.12)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (2.12.3)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (4.12.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.47.0 in /usr/local/lib/python3.12/dist-packages (from google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (2.47.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (0.28.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (2.32.4)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (9.1.4)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (4.15.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.56.0->langchain_google_genai) (1.3.1)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (0.7.1)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (26.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (6.0.3)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain_google_genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain_google_genai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (3.11)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (3.11.7)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (1.0.0)\n",
            "Requirement already satisfied: xxhash>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (3.6.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.5->langchain_google_genai) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.28.1->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (2.5.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.47.0->google-auth[requests]<3.0.0,>=2.47.0->google-genai<2.0.0,>=1.56.0->langchain_google_genai) (0.6.2)\n",
            "Downloading langchain_google_genai-4.2.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, langchain_google_genai\n",
            "Successfully installed filetype-1.2.0 langchain_google_genai-4.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4xZ-y93ua2T",
        "outputId": "da42889e-5484-4d17-f50e-a81d03c694e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your google api key:··········\n"
          ]
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your google api key:\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm_focused = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.0)\n",
        "\n",
        "llm_creative = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=1.0)"
      ],
      "metadata": {
        "id": "kaHQqncfuylY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Define the word 'idea' in one sentence.\"\n",
        "\n",
        "print(\"-- FOCUSED (Temp=0) ---\")\n",
        "print(f\"Run 1: {llm_focused.invoke(prompt).content}\")\n",
        "print(f\"Run 2: {llm_focused.invoke(prompt).content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "966s68aqwRvy",
        "outputId": "4008ce01-ccd9-4601-ffff-793375510785"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- FOCUSED (Temp=0) ---\n",
            "Run 1: An idea is a thought, concept, or mental impression that exists in the mind.\n",
            "Run 2: An idea is a thought, concept, or plan conceived in the mind.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Define the word 'idea' in one sentence.\"\n",
        "\n",
        "print(\"-- FOCUSED (Temp=0) ---\")\n",
        "print(f\"Run 1: {llm_creative.invoke(prompt).content}\")\n",
        "print(f\"Run 2: {llm_creative.invoke(prompt).content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbPIKGmAxJMB",
        "outputId": "97cebc27-202d-428d-c87a-e968f8056378"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-- FOCUSED (Temp=0) ---\n",
            "Run 1: An idea is a **thought, concept, or mental impression** formed in the mind, often serving as a plan, suggestion, or basic understanding.\n",
            "Run 2: An idea is a mental construct, such as a thought, concept, plan, or image, that is conceived in the mind.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your google api key:\")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
      ],
      "metadata": {
        "id": "U8zz8pcVxihu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content = \"AI is used for agentic AI to develop better AI models to develop better Agentic AI models. hence the AI for agentic AI is one of the most important AI concepts that exist And AI is AI.\"),\n",
        "    HumanMessage(content = \"what is AI\")\n",
        "]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeB3DYeyy3Cz",
        "outputId": "0fd8f388-ee9f-4c2f-a261-08d85a08cbb0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI stands for **Artificial Intelligence**.\n",
            "\n",
            "At its core, AI is a broad field of computer science focused on creating machines that can **perform tasks that typically require human intelligence**. This involves designing systems that can:\n",
            "\n",
            "1.  **Learn:** Acquire information and rules for using the information. This is often done through data (Machine Learning, Deep Learning).\n",
            "2.  **Reason:** Use rules to reach approximate or definite conclusions.\n",
            "3.  **Problem-solve:** Find solutions to complex challenges.\n",
            "4.  **Perceive:** Understand and interpret sensory input like images, speech, or text (Computer Vision, Natural Language Processing).\n",
            "5.  **Understand and Generate Language:** Interact with humans using natural language (NLP, Generative AI).\n",
            "6.  **Make Decisions:** Choose actions based on available information and goals.\n",
            "\n",
            "**Key Concepts and Subfields within AI:**\n",
            "\n",
            "*   **Machine Learning (ML):** A subset of AI that allows systems to learn from data without being explicitly programmed. It identifies patterns and makes predictions.\n",
            "    *   **Supervised Learning:** Learning from labeled data (e.g., recognizing cats after seeing many labeled cat images).\n",
            "    *   **Unsupervised Learning:** Finding patterns in unlabeled data (e.g., grouping customers by purchasing habits).\n",
            "    *   **Reinforcement Learning:** Learning through trial and error, receiving rewards or penalties for actions (e.g., training a game-playing AI).\n",
            "*   **Deep Learning (DL):** A subset of Machine Learning that uses artificial neural networks with multiple layers (hence \"deep\") to learn complex patterns, especially effective for images, speech, and text.\n",
            "*   **Natural Language Processing (NLP):** Enables computers to understand, interpret, and generate human language. (e.g., chatbots, translation tools, sentiment analysis).\n",
            "*   **Computer Vision (CV):** Allows computers to \"see\" and interpret visual information from images and videos. (e.g., facial recognition, self-driving cars).\n",
            "*   **Robotics:** The field of engineering that deals with the design, construction, operation, and use of robots, often incorporating AI for autonomy and decision-making.\n",
            "*   **Expert Systems:** Older AI systems that use a knowledge base and inference engine to mimic the decision-making ability of a human expert.\n",
            "*   **Generative AI:** A type of AI that can create new content, such as text, images, audio, and video, often based on patterns learned from vast datasets (e.g., ChatGPT, DALL-E).\n",
            "\n",
            "**Types of AI:**\n",
            "\n",
            "*   **Narrow AI (Weak AI):** This is the AI we have today. It's designed and trained for a specific task or a narrow set of tasks. Examples include virtual assistants (Siri, Alexa), recommendation systems (Netflix, Amazon), self-driving cars, and spam filters.\n",
            "*   **General AI (Strong AI / AGI):** This is hypothetical AI that would possess human-level cognitive abilities across a wide range of tasks, capable of understanding, learning, and applying intelligence to any intellectual task that a human can. It does not exist yet.\n",
            "*   **Superintelligence:** A hypothetical AI that would surpass human intelligence across virtually all fields, including scientific creativity, general wisdom, and social skills.\n",
            "\n",
            "**Why is AI Important?**\n",
            "\n",
            "AI is transforming nearly every industry by:\n",
            "*   **Automating tasks:** Increasing efficiency and productivity.\n",
            "*   **Solving complex problems:** From drug discovery to climate modeling.\n",
            "*   **Making better decisions:** Analyzing vast amounts of data to find insights.\n",
            "*   **Creating new capabilities:** Enabling new products and services.\n",
            "\n",
            "In essence, AI is about extending human capabilities and intelligence through machines, with its potential still largely unfolding.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your google api key:\")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "template = ChatPromptTemplate.from_template(\"Tell me a fun fact about {topic}.\")\n",
        "parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "M1RcKOCq0_DU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_value = template.invoke({\"topic\" : \"touching grass\"})\n",
        "\n",
        "response_obj = llm.invoke(prompt_value)\n",
        "\n",
        "final_text = parser.invoke(response_obj)\n",
        "\n",
        "print(final_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmEl44-Q0aJ8",
        "outputId": "ff776a57-afb3-4b9a-a3b8-7caf176070e7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The internet slang phrase \"touching grass\" – meaning to go outside, disconnect from the internet, and reconnect with reality – likely originated on imageboards like 4chan in the early 2010s.\n",
            "\n",
            "The fun (and slightly ironic) fact is that while it's often used as a sarcastic jab to tell someone to get a grip on reality, there are actual, scientifically-backed benefits to literally \"touching grass\" or spending time in nature, like:\n",
            "*   **Reducing stress hormones** (like cortisol).\n",
            "*   **Improving mood and cognitive function.**\n",
            "*   **Boosting creativity.**\n",
            "*   **Lowering blood pressure.**\n",
            "\n",
            "So, the meme is actually giving pretty good advice!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = template | llm | parser\n",
        "\n",
        "print(chain.invoke({\"topic\" : \"Octopus\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuOl5Nir2l2E",
        "outputId": "54af8e68-2fd5-4721-9df7-9c47ae24844c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's a fun one:\n",
            "\n",
            "Octopuses are incredibly squishy! They don't have a single bone in their body (not even a shell like their mollusk relatives), which means they can squeeze through incredibly tiny spaces—often no bigger than their eyeball—to escape predators or hunt for food. Imagine being able to fit through a keyhole!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment"
      ],
      "metadata": {
        "id": "TqdcQHmOMTha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Use focused model for consistency\n",
        "llm_math = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.0)\n",
        "\n",
        "movie_chain = (\n",
        "    ChatPromptTemplate.from_template(\n",
        "        \"Given the movie '{movie}', find its release year and calculate how many years ago it was from 2025. \"\n",
        "        \"Return only the final number and the year it was released.\"\n",
        "    )\n",
        "    | llm_math\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(movie_chain.invoke({\"movie\": \"Madagascar 2\"}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-xyVlUNMQGn",
        "outputId": "fabc3191-589b-4bb2-ac43-768edb7a7683"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17 2008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt engineering"
      ],
      "metadata": {
        "id": "-Vgll_DmMoYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your google api key:\")\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.0)"
      ],
      "metadata": {
        "id": "nK2NE25H3FbL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"write a rejection email to a candidate.\"\n",
        "\n",
        "print(\"... LAZY PROMPT ...\")\n",
        "print(llm.invoke(task).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OP35jM83XSA",
        "outputId": "0a3ef19f-e439-4f3c-a650-6a389efc51a9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "... LAZY PROMPT ...\n",
            "Here are a few options for a rejection email, ranging from a standard post-interview rejection to an early-stage rejection. Choose the one that best fits your situation.\n",
            "\n",
            "---\n",
            "\n",
            "**Option 1: Standard Rejection (After Interview)**\n",
            "\n",
            "This is the most common scenario, where the candidate has invested time in an interview.\n",
            "\n",
            "**Subject: Update on Your Application for [Job Title] at [Company Name]**\n",
            "\n",
            "Dear [Candidate Name],\n",
            "\n",
            "Thank you again for your interest in the [Job Title] position at [Company Name] and for taking the time to interview with our team. We truly appreciate you sharing your experience and insights with us.\n",
            "\n",
            "We had many highly qualified applicants for this role, and the decision was a difficult one. While your background and skills are impressive, we have decided to move forward with another candidate whose qualifications and experience were a closer match for our current needs.\n",
            "\n",
            "We wish you the very best in your job search and future endeavors.\n",
            "\n",
            "Sincerely,\n",
            "\n",
            "[Your Name/Hiring Team]\n",
            "[Your Title]\n",
            "[Company Name]\n",
            "\n",
            "---\n",
            "\n",
            "**Option 2: Early Stage Rejection (After Application, Before Interview)**\n",
            "\n",
            "Use this when you've reviewed applications but decided not to move forward with an interview.\n",
            "\n",
            "**Subject: Your Application for [Job Title] at [Company Name]**\n",
            "\n",
            "Dear [Candidate Name],\n",
            "\n",
            "Thank you for your interest in the [Job Title] position at [Company Name] and for submitting your application. We appreciate you taking the time to apply.\n",
            "\n",
            "We received a large number of applications for this role, and after careful consideration, we've decided to move forward with candidates whose qualifications and experience more closely align with the specific requirements of this position at this time.\n",
            "\n",
            "We wish you the best of luck in your job search and future career.\n",
            "\n",
            "Sincerely,\n",
            "\n",
            "[Your Name/Hiring Team]\n",
            "[Your Title]\n",
            "[Company Name]\n",
            "\n",
            "---\n",
            "\n",
            "**Option 3: Rejection with an Offer to Keep Resume on File (For strong candidates who weren't the *perfect* fit for *this* role)**\n",
            "\n",
            "**Subject: Update on Your Application for [Job Title] at [Company Name]**\n",
            "\n",
            "Dear [Candidate Name],\n",
            "\n",
            "Thank you again for your interest in the [Job Title] position at [Company Name] and for taking the time to interview with our team. We truly appreciate you sharing your experience and insights with us.\n",
            "\n",
            "We had many highly qualified applicants for this role, and the decision was a difficult one. While your background and skills are impressive, and we enjoyed learning more about you, we have decided to move forward with another candidate whose qualifications and experience were a closer match for our current needs.\n",
            "\n",
            "We were very impressed with [mention a general positive, e.g., \"your enthusiasm\" or \"your experience in X area\"] and would like to keep your resume on file for future openings that may be a better fit.\n",
            "\n",
            "We wish you the very best in your job search and future endeavors.\n",
            "\n",
            "Sincerely,\n",
            "\n",
            "[Your Name/Hiring Team]\n",
            "[Your Title]\n",
            "[Company Name]\n",
            "\n",
            "---\n",
            "\n",
            "**Key Considerations for Rejection Emails:**\n",
            "\n",
            "*   **Be Prompt:** Send rejections in a timely manner.\n",
            "*   **Be Clear and Concise:** Get straight to the point respectfully.\n",
            "*   **Be Professional and Courteous:** Always maintain a positive company image.\n",
            "*   **Avoid Specific Reasons:** Do not give detailed feedback or specific reasons for rejection (e.g., \"you lacked X skill,\" \"your answers were weak\"). This can open the door to legal challenges or unnecessary debate. Stick to general statements like \"closer match\" or \"specific requirements.\"\n",
            "*   **Personalize (Slightly):** Use the candidate's name and the specific job title.\n",
            "*   **Proofread:** Ensure there are no typos or grammatical errors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "structured_prompt = \"\"\"**Option 1: Standard Rejection (After Interview)**\n",
        "\n",
        "This is the most common scenario, where the candidate has invested time in an interview.\n",
        "\n",
        "**Subject: Update on Your Application for [Job Title] at [Company Name]**\n",
        "\n",
        "Dear [Candidate Name],\n",
        "\n",
        "Thank you again for your interest in the [Job Title] position at [Company Name] and for taking the time to interview with our team. We truly appreciate you sharing your experience and insights with us.\n",
        "\n",
        "We had many highly qualified applicants for this role, and the decision was a difficult one. While your background and skills are impressive, we have decided to move forward with another candidate whose qualifications and experience were a closer match for our current needs.\n",
        "\n",
        "We wish you the very best in your job search and future endeavors.\n",
        "\n",
        "Sincerely,\n",
        "\n",
        "[Your Name/Hiring Team]\n",
        "[Your Title]\n",
        "[Company Name]\"\"\"\n",
        "\n",
        "print(\"--- STRUCTURED PROMPT ---\")\n",
        "print(llm.invoke(structured_prompt).content + \"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOZoNR-A3hmU",
        "outputId": "e7f64797-0cbf-4ff5-9679-ff9d1ab95c8b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- STRUCTURED PROMPT ---\n",
            "This is a **very solid and professional standard rejection letter** after an interview. It hits all the right notes for politeness, clarity, and legal safety.\n",
            "\n",
            "Here's a breakdown of its strengths and a minor suggestion for enhancement:\n",
            "\n",
            "---\n",
            "\n",
            "**Strengths:**\n",
            "\n",
            "1.  **Clear and Professional Subject Line:** Immediately informs the candidate about the email's purpose without being overly harsh.\n",
            "2.  **Expresses Gratitude:** Starts by thanking the candidate for their interest and, crucially, for their time and effort in interviewing. \"Truly appreciate you sharing your experience and insights\" is a nice touch that acknowledges their investment.\n",
            "3.  **Softens the Blow:** Phrases like \"many highly qualified applicants\" and \"the decision was a difficult one\" help to convey that the candidate was competitive and the decision wasn't easy.\n",
            "4.  **Compliments the Candidate:** Stating \"While your background and skills are impressive\" is important after an interview. It validates their effort and helps maintain a positive impression of your company.\n",
            "5.  **Provides a Vague, Safe Reason:** \"Closer match for our current needs\" is the gold standard for rejection reasons. It's truthful without being overly specific (which can lead to debate or legal issues) and doesn't imply any deficiency on the candidate's part.\n",
            "6.  **Professional Closing:** Wishes them well in their future endeavors, maintaining a positive final interaction.\n",
            "7.  **Concise:** Gets straight to the point without unnecessary fluff.\n",
            "\n",
            "---\n",
            "\n",
            "**Minor Suggestion for Enhancement (Optional):**\n",
            "\n",
            "While the current version is excellent, you could add a very subtle, general encouragement for future applications if you'd like to keep strong candidates in your talent pool.\n",
            "\n",
            "**Enhanced Version (with optional addition):**\n",
            "\n",
            "**Subject: Update on Your Application for [Job Title] at [Company Name]**\n",
            "\n",
            "Dear [Candidate Name],\n",
            "\n",
            "Thank you again for your interest in the [Job Title] position at [Company Name] and for taking the time to interview with our team. We truly appreciate you sharing your experience and insights with us.\n",
            "\n",
            "We had many highly qualified applicants for this role, and the decision was a difficult one. While your background and skills are impressive, we have decided to move forward with another candidate whose qualifications and experience were a closer match for our current needs.\n",
            "\n",
            "We encourage you to keep an eye on our careers page for future opportunities that may align with your skills and experience.\n",
            "\n",
            "We wish you the very best in your job search and future endeavors.\n",
            "\n",
            "Sincerely,\n",
            "\n",
            "[Your Name/Hiring Team]\n",
            "[Your Title]\n",
            "[Company Name]\n",
            "\n",
            "---\n",
            "\n",
            "**Conclusion:**\n",
            "\n",
            "Your original \"Option 1\" is **excellent and perfectly suitable** as is. The suggested enhancement is purely optional and depends on whether your company wants to subtly encourage re-application from strong candidates. You've crafted a respectful, clear, and professional rejection letter.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm_code = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.3)\n",
        "\n",
        "code_prompt = ChatPromptTemplate.from_template(\n",
        "\"\"\"\n",
        "Context: You are a Senior Python Dev.\n",
        "Objective: Write a function to reverse a string.\n",
        "Constraint: It must use recursion (no slicing [::-1]).\n",
        "Style: Include detailed docstrings.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "code_chain = code_prompt | llm_code | StrOutputParser()\n",
        "\n",
        "print(code_chain.invoke({}))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wjQ5VPdNClt",
        "outputId": "0b4ba5f2-634a-43b5-e8f5-0a330e84f34c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As a Senior Python Dev, I'd approach this by focusing on clarity, efficiency (within the recursive constraint), and comprehensive documentation.\n",
            "\n",
            "Here's the function:\n",
            "\n",
            "```python\n",
            "def reverse_string_recursive(s: str) -> str:\n",
            "    \"\"\"\n",
            "    Reverses a given string using recursion.\n",
            "\n",
            "    This function takes a string and returns a new string with the characters\n",
            "    in reverse order. It adheres strictly to the recursive paradigm, defining\n",
            "    a base case and a recursive step to break down the problem.\n",
            "\n",
            "    Constraint Adherence:\n",
            "    - Uses recursion.\n",
            "    - Explicitly avoids string slicing like `[::-1]`.\n",
            "\n",
            "    How it works:\n",
            "    1.  **Base Case:** If the input string is empty or contains only one character,\n",
            "        it's already \"reversed\" in its simplest form, so we return it directly.\n",
            "        This is the termination condition for the recursion.\n",
            "    2.  **Recursive Step:** For any string longer than one character, we perform\n",
            "        two main operations:\n",
            "        a.  We take the *first* character of the string (`s[0]`).\n",
            "        b.  We recursively call the function on the *rest* of the string (`s[1:]`).\n",
            "            This effectively shortens the string for the next recursive call.\n",
            "        c.  The magic happens when we combine the results: we append the first\n",
            "            character (`s[0]`) to the result of reversing the rest of the string.\n",
            "            This builds the reversed string from the \"inside out\" (or rather,\n",
            "            from the last character to the first).\n",
            "\n",
            "    Example Trace for \"abc\":\n",
            "    reverse_string_recursive(\"abc\")\n",
            "    -> reverse_string_recursive(\"bc\") + 'a'\n",
            "       -> reverse_string_recursive(\"c\") + 'b'\n",
            "          -> reverse_string_recursive(\"\") + 'c'  (Base Case: returns \"\")\n",
            "          -> \"\" + 'c'  (returns \"c\")\n",
            "       -> \"c\" + 'b'  (returns \"cb\")\n",
            "    -> \"cb\" + 'a'  (returns \"cba\")\n",
            "\n",
            "    Args:\n",
            "        s (str): The input string to be reversed.\n",
            "\n",
            "    Returns:\n",
            "        str: The reversed string.\n",
            "\n",
            "    Raises:\n",
            "        TypeError: If the input `s` is not a string. (Implicitly handled by Python's\n",
            "                   string operations, but good to consider in a real-world scenario\n",
            "                   where explicit type checking might be added).\n",
            "\n",
            "    Examples:\n",
            "        >>> reverse_string_recursive(\"hello\")\n",
            "        'olleh'\n",
            "        >>> reverse_string_recursive(\"Python\")\n",
            "        'nohtyP'\n",
            "        >>> reverse_string_recursive(\"\")\n",
            "        ''\n",
            "        >>> reverse_string_recursive(\"a\")\n",
            "        'a'\n",
            "        >>> reverse_string_recursive(\"racecar\")\n",
            "        'racecar'\n",
            "    \"\"\"\n",
            "    # Type checking for robustness (optional, but good practice for a senior dev)\n",
            "    if not isinstance(s, str):\n",
            "        raise TypeError(\"Input must be a string.\")\n",
            "\n",
            "    # Base Case: An empty string or a single-character string is its own reverse.\n",
            "    if len(s) <= 1:\n",
            "        return s\n",
            "    else:\n",
            "        # Recursive Step:\n",
            "        # Take the first character (s[0])\n",
            "        # Recursively reverse the rest of the string (s[1:])\n",
            "        # Append the first character to the result of the recursive call.\n",
            "        return reverse_string_recursive(s[1:]) + s[0]\n",
            "\n",
            "# --- Demonstration ---\n",
            "if __name__ == \"__main__\":\n",
            "    print(f\"'hello' reversed: {reverse_string_recursive('hello')}\")\n",
            "    print(f\"'Python' reversed: {reverse_string_recursive('Python')}\")\n",
            "    print(f\"'' reversed: {reverse_string_recursive('')}\")\n",
            "    print(f\"'a' reversed: {reverse_string_recursive('a')}\")\n",
            "    print(f\"'racecar' reversed: {reverse_string_recursive('racecar')}\")\n",
            "    print(f\"'Madam, I'm Adam' reversed: {reverse_string_recursive('Madam, I\\'m Adam')}\")\n",
            "\n",
            "    # Example of potential TypeError\n",
            "    try:\n",
            "        reverse_string_recursive(123)\n",
            "    except TypeError as e:\n",
            "        print(f\"\\nCaught expected error: {e}\")\n",
            "\n",
            "    # Note on recursion depth:\n",
            "    # For very long strings, this recursive approach can hit Python's default\n",
            "    # recursion limit (usually 1000 or 3000). For practical applications with\n",
            "    # potentially very long strings, an iterative approach is generally preferred\n",
            "    # for string reversal to avoid StackOverflowError. However, for the purpose\n",
            "    # of demonstrating recursion, this is the canonical implementation.\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.5)"
      ],
      "metadata": {
        "id": "4sWil-Cx5KU3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_zero = \"Combine 'Angry' and 'Hungry' into a funny new word.\"\n",
        "print(f\"Zero-Shot: {llm.invoke(prompt_zero).content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDVuBixKDLxx",
        "outputId": "05762841-87b3-4867-997a-7d039c2fa167"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-Shot: The most common and widely accepted funny word for this is:\n",
            "\n",
            "**Hangry**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_few = \"\"\"\n",
        "Combine words into a funny new word. Give a sarcastic definition.\n",
        "\n",
        "Input: Breakfast + Lunch\n",
        "Output: Brunch (An excuse to drink alcohol before noon)\n",
        "\n",
        "Input: Chill + Relax\n",
        "Output: Chillax (What annoying people say when you are panic attacks)\n",
        "\n",
        "Input: Angry + Hungry\n",
        "Output:\n",
        "\"\"\"\n",
        "print(f\"Few-Shot: {llm.invoke(prompt_few).content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sk4A-LxTDQJY",
        "outputId": "653628e4-ce2f-4106-8ac0-c8fcf603fb8c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Few-Shot: Output: Hangry (The only scientifically recognized condition that justifies your irrational rage before your next meal.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
      ],
      "metadata": {
        "id": "P1H6guyqDQ2X"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
        "\n",
        "examples = [\n",
        "    {\"input\": \"The internet is down.\", \"output\": \"We are observing connectivity latency.\"},\n",
        "    {\"input\": \"This code implies a bug.\", \"output\": \"The logic suggests unintended behavior.\"},\n",
        "    {\"input\": \"I hate this feature.\", \"output\": \"This feature does not align with my preferences.\"},\n",
        "]\n",
        "\n",
        "\n",
        "example_fmt = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"{input}\"),\n",
        "    (\"ai\", \"{output}\")\n",
        "])\n",
        "\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_fmt,\n",
        "    examples=examples\n",
        ")\n",
        "\n",
        "\n",
        "final_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a Corpo-Speak Translator. Rewrite the input to sound professional.\"),\n",
        "    few_shot_prompt,\n",
        "    (\"human\", \"{text}\")\n",
        "])\n",
        "\n",
        "chain = final_prompt | llm\n",
        "\n",
        "print(chain.invoke({\"text\": \"This app sucks.\"}).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vj7GdxjODTkd",
        "outputId": "d11f252e-9159-40cb-92e9-7f7e43d1e465"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The application's current state presents opportunities for enhancement.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced prompt engineering"
      ],
      "metadata": {
        "id": "wKZRTjG2M3Jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
        "\n",
        "# Using Llama3.1-8b (Small/Fast) to demonstrate logic failures\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmqJjpRMDZxi",
        "outputId": "089a23b1-b7b0-480a-a80d-0616df228319"
      },
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hEnter your Groq API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many does he have now?\"\n",
        "\n",
        "prompt_standard = f\"Answer this question: {question}\"\n",
        "print(\"--- STANDARD (Llama3.1-8b) ---\")\n",
        "print(llm.invoke(prompt_standard).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pt0AVzCDxKc",
        "outputId": "ed7aedfe-fa2a-4275-d291-229376c184ff"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- STANDARD (Llama3.1-8b) ---\n",
            "To find out how many tennis balls Roger has now, we need to add the initial number of tennis balls he had (5) to the number of tennis balls he bought (2 cans * 3 tennis balls per can).\n",
            "\n",
            "2 cans * 3 tennis balls per can = 6 tennis balls\n",
            "\n",
            "Now, let's add the initial number of tennis balls (5) to the number of tennis balls he bought (6):\n",
            "\n",
            "5 + 6 = 11\n",
            "\n",
            "So, Roger now has 11 tennis balls.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_cot = f\"Answer this question. Let's think step by step. {question}\"\n",
        "\n",
        "print(\"--- Chain of Thought (Llama3.1-8b) ---\")\n",
        "print(llm.invoke(prompt_cot).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MCSvmCTDz1u",
        "outputId": "13eaf552-8bc7-4ff0-e18e-058db8c011cc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Chain of Thought (Llama3.1-8b) ---\n",
            "To find out how many tennis balls Roger has now, we need to follow these steps:\n",
            "\n",
            "1. Roger already has 5 tennis balls.\n",
            "2. He buys 2 more cans of tennis balls. Each can has 3 tennis balls, so he buys 2 x 3 = 6 more tennis balls.\n",
            "3. Now, we add the tennis balls he already had (5) to the tennis balls he bought (6). 5 + 6 = 11\n",
            "\n",
            "So, Roger now has 11 tennis balls.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
        "\n",
        "# Using Llama3.1-8b\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.7)"
      ],
      "metadata": {
        "id": "gpnwDleID2DW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "problem = \"How can I get my 5-year-old to eat vegetables?\"\n",
        "\n",
        "# Step 1: The Branch Generator\n",
        "prompt_branch = ChatPromptTemplate.from_template(\n",
        "    \"Problem: {problem}. Give me one unique, creative solution. Solution {id}:\"\n",
        ")\n",
        "\n",
        "branches = RunnableParallel(\n",
        "    sol1=prompt_branch.partial(id=\"1\") | llm | StrOutputParser(),\n",
        "    sol2=prompt_branch.partial(id=\"2\") | llm | StrOutputParser(),\n",
        "    sol3=prompt_branch.partial(id=\"3\") | llm | StrOutputParser(),\n",
        ")\n",
        "\n",
        "# Step 2: The Judge\n",
        "prompt_judge = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    I have three proposed solutions for: '{problem}'\n",
        "\n",
        "    1: {sol1}\n",
        "    2: {sol2}\n",
        "    3: {sol3}\n",
        "\n",
        "    Act as a Child Psychologist. Pick the most sustainable one (not bribery) and explain why.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Chain: Input -> Branches -> Judge -> Output\n",
        "tot_chain = (\n",
        "    RunnableParallel(problem=RunnableLambda(lambda x: x), branches=branches)\n",
        "    | (lambda x: {**x[\"branches\"], \"problem\": x[\"problem\"]})\n",
        "    | prompt_judge\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"--- Tree of Thoughts (ToT) Result ---\")\n",
        "print(tot_chain.invoke(problem))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WBl_E01D9bQ",
        "outputId": "a4b18fda-1881-45b4-956b-0e4496841a92"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tree of Thoughts (ToT) Result ---\n",
            "As a child psychologist, my recommendation for the most sustainable solution that is not bribery is **Solution 2: \"Veggie Superhero\"**.\n",
            "\n",
            "This solution stands out for several reasons:\n",
            "\n",
            "1. **Engagement and Imagination**: The \"Veggie Superhero\" approach sparks imagination and creativity in children. It encourages them to think creatively and engage with the mealtime experience, which is essential for developing healthy eating habits.\n",
            "2. **Storytelling and Narrative**: Creating a storyline around vegetables helps children associate them with positive experiences and emotions. This narrative approach can lead to a deeper understanding of the importance of healthy eating and a more positive relationship with vegetables.\n",
            "3. **Intrinsic Motivation**: By making mealtime a fun and engaging experience, children are more likely to develop an intrinsic motivation to eat vegetables. This means they will be motivated to eat vegetables because they enjoy the experience, rather than because of external rewards or bribes.\n",
            "4. **Flexibility and Adaptability**: The \"Veggie Superhero\" approach can be adapted to different ages and personalities, making it a versatile solution for families with children of varying needs and interests.\n",
            "5. **Long-term Impact**: This solution has the potential for long-term impact, as it teaches children a valuable skill (creative problem-solving) and helps them develop a positive relationship with healthy eating habits.\n",
            "\n",
            "While the other solutions (Veggie Face Food Art, Rainbow Plate, and another Veggie Face solution) can be effective in the short term, they may not be as sustainable in the long term. Children may eventually become bored with the novelty of food art or the repetition of a particular theme. In contrast, the \"Veggie Superhero\" approach encourages creativity, imagination, and storytelling, making it a more sustainable solution for promoting healthy eating habits in children.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_draft = ChatPromptTemplate.from_template(\n",
        "    \"Write a 1-sentence movie plot about: {topic}. Genre: {genre}.\"\n",
        ")\n",
        "\n",
        "drafts = RunnableParallel(\n",
        "    draft_scifi=prompt_draft.partial(genre=\"Sci-Fi\") | llm | StrOutputParser(),\n",
        "    draft_romance=prompt_draft.partial(genre=\"Romance\") | llm | StrOutputParser(),\n",
        "    draft_horror=prompt_draft.partial(genre=\"Horror\") | llm | StrOutputParser(),\n",
        ")\n",
        "\n",
        "prompt_combine = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    I have three movie ideas for the topic '{topic}':\n",
        "    1. Sci-Fi: {draft_scifi}\n",
        "    2. Romance: {draft_romance}\n",
        "    3. Horror: {draft_horror}\n",
        "\n",
        "    Your task: Create a new Mega-Movie that combines the TECHNOLOGY of Sci-Fi, the PASSION of Romance, and the FEAR of Horror.\n",
        "    Write one paragraph.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "got_chain = (\n",
        "    RunnableParallel(topic=RunnableLambda(lambda x: x), drafts=drafts)\n",
        "    | (lambda x: {**x[\"drafts\"], \"topic\": x[\"topic\"]})\n",
        "    | prompt_combine\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"--- Graph of Thoughts (GoT) Result ---\")\n",
        "print(got_chain.invoke(\"Time Travel\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QWfFR9LD-Mi",
        "outputId": "402d6b1a-578f-4ea3-ff0a-ea6a92fbc9a1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Graph of Thoughts (GoT) Result ---\n",
            "In \"Echoes of Eternity,\" a brilliant physicist, Emma, discovers a groundbreaking technology that allows her to manipulate time and space, but at a terrible cost: each time she travels back in time, she creates a new parallel universe, and with it, a new version of herself. As she tries to prevent a catastrophic event that will destroy the fabric of reality, she becomes obsessed with reuniting with her high school sweetheart, Jack, who is now trapped in a different timeline. But as she navigates through the ever-changing multiverse, she realizes that her actions are attracting a malevolent force that follows her across dimensions, threatening to destroy not only her relationships but also the very fabric of existence. With each step back in time, Emma must confront her own darker self and the consequences of her actions, all while racing against time to save the one she loves and prevent the impending apocalypse.\n"
          ]
        }
      ]
    }
  ]
}